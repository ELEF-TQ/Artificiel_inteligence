{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier tp porte sur la régression linéaire. La régression linéaire tente d'ajuster une droite de meilleure approximation à un ensemble de données, en utilisant une ou plusieurs caractéristiques comme coefficients d'une équation linéaire. Ici, Nous allons vais discuter :\n",
    "\n",
    " - Le chargement, la manipulation et le traçage de données à l'aide de numpy et matplotlib\n",
    " - Les fonctions d'hypothèse et de coût pour la régression linéaire\n",
    " - La descente de gradient avec une variable et plusieurs variables\n",
    " - Mise à l'échelle et normalisation des caractéristiques\n",
    " - Vectorisation et équation normale\n",
    " - Régression linéaire  dans sk-learn\n",
    " - Régression linéaire et descente de gradient dans Tensorflow\n",
    " \n",
    "Nous utilisons comme données d'entainement le jeu de données [UCI Bike Sharing Data Set](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement et traçage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la première partie, nous allons effectuer une régression linéaire avec une variable, et nous n'utiliserons donc que deux champs de l'ensemble de données quotidiennes : la température maximale normalisée en °C et le nombre total de locations de vélos. Les valeurs des locations sont mises à l'échelle par un facteur de mille, compte tenu de la différence de magnitude entre elles et les températures normalisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./data.csv\")\n",
    "print(data.head(5))\n",
    "temps   = data['atemp'].values\n",
    "rentals = data['cnt'].values / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique révèle un certain degré de corrélation entre la température et la location de vélos, comme on pourrait le deviner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(temps, rentals, marker='x', color='red')\n",
    "plt.xlabel('Normalized Temperature in C')\n",
    "plt.ylabel('Bike Rentals in 1000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression linéaire simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commencerons par implémenter la fonction de coût ([cost function](https://en.wikipedia.org/wiki/Loss_function) ) pour la régression linéaire, plus précisément l'erreur quadratique moyenne ([mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) MSE). Intuitivement, l'erreur quadratique moyenne représente une agrégation des distances entre la valeur y réelle du point et ce qu'une fonction d'hypothèse $h_\\theta(x)$ prédit. Cette fonction d'hypothèse et la fonction de coût $J(\\theta)$ sont définies comme\n",
    "\n",
    "\\begin{align}\n",
    "h_\\theta(x) & = \\theta_0 + \\theta_1x_1 \\\\\n",
    "J(\\theta) & = \\frac{1}{2m}\\sum\\limits_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "\\end{align}\n",
    "\n",
    "où $\\theta$ est un vecteur de pondérations de caractéristiques, $x^{(i)}$ est le ième exemple d'apprentissage, $y^{(i)}$ est la valeur y de cet exemple et $x_j$ est la valeur de sa jième caractéristique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    return np.sum(np.square(np.matmul(X, theta) - y)) / (2 * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de calculer le coût avec une estimation initiale de $\\theta$, une colonne de 1 est ajoutée aux données d'entrée. Cela nous permet de vectoriser la fonction de coût, ainsi que de la rendre utilisable ultérieurement pour une régression linéaire multiple. Cette première valeur $\\theta_0$ se comporte désormais comme une constante dans la fonction de coût."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(2)\n",
    "X = np.column_stack((np.ones(len(temps)), temps))\n",
    "y = rentals\n",
    "print(X[:5])\n",
    "print(y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost = compute_cost(X, y, theta)\n",
    "\n",
    "print('theta:', theta)\n",
    "print('cost:', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant minimiser le coût en utilisant l'algorithme de [descente de gradient](https://en.wikipedia.org/wiki/Gradient_descent). Intuitivement, la descente de gradient prend de petits sauts linéaires le long de la pente d'une fonction dans chaque dimension de caractéristique, la taille de chaque saut étant déterminée par la dérivée partielle de la fonction de coût par rapport à cette caractéristique et un multiplicateur de taux d'apprentissage $\\alpha$. Si l'algorithme est correctement réglé, il converge vers un minimum global en ajustant de manière itérative les poids des caractéristiques $\\theta$ de la fonction de coût, comme indiqué ici pour deux dimensions de caractéristiques :\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_0 & := \\theta_0 - \\alpha\\frac{\\partial}{\\partial\\theta_0} J(\\theta_0,\\theta_1) \\\\\n",
    "\\theta_1 & := \\theta_1 - \\alpha\\frac{\\partial}{\\partial\\theta_1} J(\\theta_0,\\theta_1)\n",
    "\\end{align}\n",
    "\n",
    "La règle de mise à jour à chaque itération devient alors :\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_0 & := \\theta_0 - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)}) \\\\\n",
    "\\theta_1 & := \\theta_1 - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_1^{(i)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Voir [ici](http://mccormickml.com/2014/03/04/gradient-descent-derivation/) pour une explication plus détaillée de la manière dont les équations de mise à jour sont dérivées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha, max_iter):\n",
    "    theta = np.zeros(2)\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        t0 = theta[0] - (alpha / m) * np.sum(np.dot(X, theta) - y)\n",
    "        t1 = theta[1] - (alpha / m) * np.sum((np.dot(X, theta) - y) * X[:,1])\n",
    "        theta = np.array([t0, t1])\n",
    "\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_iter = 5000\n",
    "alpha = 0.1\n",
    "\n",
    "theta = gradient_descent(X, y, alpha, iterations)\n",
    "cost = compute_cost(X, y, theta)\n",
    "\n",
    "print(\"theta:\", theta)\n",
    "print('cost:', compute_cost(X, y, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons examiner les valeurs de $\\theta$ choisies par l'algorithme à l'aide de quelques visualisations différentes, en traçant d'abord $h_\\theta(x)$ par rapport aux données d'entrée. Les résultats montrent la corrélation attendue entre la température et les loyers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(temps, rentals, marker='x', color='red')\n",
    "plt.xlabel('Normalized Temperature in C')\n",
    "plt.ylabel('Bike Rentals in 1000s')\n",
    "samples = np.linspace(min(temps), max(temps))\n",
    "plt.plot(samples, theta[0] + theta[1] * samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tracé de surface est une meilleure illustration de la façon dont la descente de gradient s'approche d'un minimum global, en traçant les valeurs de $\\theta$ par rapport à leur coût associé. Cela nécessite un peu plus de code qu'une implémentation dans Octave/MATLAB, en grande partie à cause de la façon dont les données d'entrée sont générées et transmises à la fonction de tracé de surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "Xs, Ys = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-40, 40, 50))\n",
    "Zs = np.array([compute_cost(X, y, [t0, t1]) for t0, t1 in zip(np.ravel(Xs), np.ravel(Ys))])\n",
    "Zs = np.reshape(Zs, Xs.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.gca(projection=\"3d\")\n",
    "ax.set_xlabel(r't0')\n",
    "ax.set_ylabel(r't1')\n",
    "ax.set_zlabel(r'cost')\n",
    "ax.view_init(elev=25, azim=40)\n",
    "ax.plot_surface(Xs, Ys, Zs, cmap=cm.rainbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, un tracé de contour révèle des tranches de ce tracé de surface dans l'espace 2D et peut montrer les valeurs $\\theta$ résultantes se situant exactement au minimum global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().gca()\n",
    "ax.plot(theta[0], theta[1], 'r*')\n",
    "plt.contour(Xs, Ys, Zs, np.logspace(-3, 3, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression linéaire par SKlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection des données\n",
    "\n",
    "- séparer le dataset en training set et test set (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = temps\n",
    "y = rentals\n",
    "y=y.reshape(len(y), 1)\n",
    "X=X.reshape(len(X), 1)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - importer le model lineaire et faire l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage du score R2\n",
    "\n",
    " Le R2 score, aussi appelé R-squared ou coefficient de détermination, est l’une des métriques les plus utilisées pour la régression linéaire. Cette métrique est une version “normalisée” de la MSE (Mean Squared Error).\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = lr.score(X_test, y_test) # renvoie le R2.\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "y_pred = lr.predict(X)\n",
    "\n",
    "plt.scatter(X, y, marker='+', color='b')\n",
    "plt.scatter(X, y_pred, marker='o', color='r')\n",
    " \n",
    "plt.xlabel('Normalized Temperature in C')\n",
    "plt.ylabel('Bike Rentals in 1000s') \n",
    "\n",
    "print (\"intercept: \" ,lr.intercept_)\n",
    "print (\"coef: \", lr.coef_)\n",
    "print (\"Score by LR: \", score)\n",
    "print (\"R2 score: \", metrics.r2_score(y_test, lr.predict(X_test)))\n",
    "#https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "\n",
    "theta  =np.hstack( (lr.intercept_ , lr.coef_[0]))   \n",
    "X = np.column_stack((np.ones(len(temps)), temps))\n",
    " \n",
    "print(\"theta:\", theta)\n",
    "print('cost:', compute_cost(X, rentals , theta))  \n",
    "print(\"MSE: \", np.sqrt(metrics.mean_squared_error(y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression linéaire par SKlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous rechargeons les données et ajoutons deux autres caractéristiques, l'humidité et la vitesse du vent.\n",
    "\n",
    "Avant d'implémenter la descente de gradient pour plusieurs variables, nous appliquerons également la [mise à l'échelle des caractéristiques](https://en.wikipedia.org/wiki/Feature_scaling) pour normaliser les valeurs des caractéristiques, empêchant l'une d'entre elles d'influencer de manière disproportionnée les résultats, ainsi que pour aider la descente de gradient à converger plus rapidement. Dans ce cas, chaque valeur de caractéristique est ajustée en soustrayant la moyenne et en divisant le résultat par l'écart type de toutes les valeurs de cette caractéristique :\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Vous trouverez plus de détails sur la mise à l'échelle et la normalisation des caractéristiques [ici](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(X):\n",
    "    n_features = X.shape[1]\n",
    "    means = np.array([np.mean(X[:,i]) for i in range(n_features)])\n",
    "    stddevs = np.array([np.std(X[:,i]) for i in range(n_features)])\n",
    "    normalized = (X - means) / stddevs\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "X = data[['atemp', 'hum', 'windspeed']].values\n",
    "X = feature_normalize(X)\n",
    "X = np.column_stack((np.ones(len(X)), X))\n",
    "\n",
    "y = data['cnt'].values / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étape suivante consiste à implémenter la descente de gradient pour un nombre quelconque de fonctionnalités. Heureusement, l'étape de mise à jour se généralise facilement et peut être vectorisée pour éviter d'itérer sur les valeurs $\\theta_j$ comme cela pourrait être suggéré par l'implémentation à variable unique ci-dessus :\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_multi(X, y, theta, alpha, iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    m = len(X)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        gradient = (1/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "theta = gradient_descent_multi(X, y, theta, alpha, iterations)\n",
    "cost = compute_cost(X, y, theta)\n",
    "\n",
    "print('theta:', theta)\n",
    "print('cost', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Malheureusement, il est désormais plus difficile d'évaluer les résultats visuellement, mais nous pouvons les vérifier avec une méthode totalement différente de calcul de la réponse, l'[équation normale](http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/). Cela résout directement la solution sans itération spécifiant une valeur $\\alpha$, bien qu'elle commence à être moins performante que la descente de gradient avec un grand nombre (plus de 10 000) de fonctionnalités.\n",
    "\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def normal_eq(X, y):\n",
    "    return inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "theta = normal_eq(X, y)\n",
    "cost = compute_cost(X, y, theta)\n",
    "\n",
    "print('theta:', theta)\n",
    "print('cost:', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs $\\theta$ et les coûts de chaque implémentation sont identiques, nous pouvons donc avoir un degré élevé de confiance dans leur exactitude.\n",
    "\n",
    "## Régression linéaire dans Tensorflow\n",
    "\n",
    "Tensorflow propose des abstractions de niveau nettement supérieur avec lesquelles travailler, représentant l'algorithme sous forme de graphique de calcul. Il dispose d'un optimiseur de descente de gradient intégré qui peut minimiser la fonction de coût sans que nous ayons à définir le gradient manuellement.\n",
    "\n",
    "Nous commencerons par recharger les données et les adapter à des structures de données et à une terminologie plus adaptées à Tensorflow. Les fonctionnalités sont toujours normalisées comme avant, mais la colonne ajoutée de 1 est absente : la constante est traitée séparément comme une variable *biais*, les valeurs $\\theta$ précédentes sont désormais des *poids*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "X = data[['atemp', 'hum', 'windspeed']].values\n",
    "X = feature_normalize(X)\n",
    "y = data['cnt'].values / 1000\n",
    "y = y.reshape((-1, 1))\n",
    "\n",
    "m = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "examples = tf.placeholder(tf.float32, [m,n])\n",
    "labels = tf.placeholder(tf.float32, [m,1])\n",
    "weights = tf.Variable(tf.zeros([n,1], dtype=np.float32), name='weight')\n",
    "bias = tf.Variable(tf.zeros([1], dtype=np.float32), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La descente du gradient complète se déroule ci-dessous en seulement trois lignes de code. Il suffit de définir les fonctions d'hypothèse et de coût, puis un optimiseur de descente du gradient pour trouver le minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.add(tf.matmul(examples, weights), bias)\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - y)) / (2 * m)\n",
    "optimizer = tf.train.GradientDescentOptimizer(alpha).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique est maintenant prêt à être utilisé, et il ne reste plus qu'à démarrer une session, exécuter l'optimiseur de manière itérative et vérifier les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(1, iterations):\n",
    "        sess.run(optimizer, feed_dict={\n",
    "            examples: X,\n",
    "            labels: y\n",
    "        }) \n",
    "        \n",
    "    print('bias:', sess.run(bias))\n",
    "    print('weights:', sess.run(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs de biais et de pondération sont identiques aux valeurs $\\theta$ calculées dans les deux implémentations précédemment, donc l'implémentation Tensorflow de l'algorithme semble correcte.\n",
    "\n",
    "You can find the original IPython notebook for this post on [GitHub](https://github.com/crsmithdev/notebooks/blob/master/ml-linear-regression/ml-linear-regression.ipynb)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
